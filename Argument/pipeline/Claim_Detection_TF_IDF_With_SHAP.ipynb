{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/Users/johanneswidera/Uni/bachelorarbeit/Code/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from helper import load_csv\n",
    "from model_helper.tf_idf import build_tf_idf\n",
    "from model_helper.PipelineWrapper import PipelineWrapper\n",
    "from custom_shap_explainer.custom_global import custom_global_explanation, custom_global_shap_distribution\n",
    "from custom_shap_explainer.signal_words import highlight_signal_words\n",
    "np.random.seed(1337)\n",
    "shap.initjs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "  text = text.lower()\n",
    "  text = contractions.fix(text)\n",
    "  text = text.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
    "  text = re.sub(' +', ' ', text)\n",
    "  return text \n",
    "\n",
    "def load_data(source):\n",
    "  data = pd.read_csv(source)\n",
    "  # select only 2 columns\n",
    "  data = data[['candidate', 'label']]\n",
    "  # rename columns\n",
    "  data.columns = ['text', 'label']\n",
    "  data['text'] = data['text'].apply(preprocessor)\n",
    "  return data['text'].tolist(), data['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train, y_train = load_data('../data/train.csv')\n",
    "corpus_test, y_test = load_data('../data/test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tf_idf = build_tf_idf()\n",
    "vectorizer_tf_idf.fit(corpus_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Build Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(penalty=\"l2\")\n",
    "model_logregression_new = PipelineWrapper(model, vectorizer_tf_idf, corpus_test, corpus_train, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logregression_new.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logregression_new.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_logreg = model_logregression_new.predict_proba(corpus_test)\n",
    "\n",
    "\n",
    "LOGREG_PREDICTION_FILE = 'predictions_logreg.csv'\n",
    "\n",
    "file_exists = os.path.exists(LOGREG_PREDICTION_FILE)\n",
    "\n",
    "if not file_exists:\n",
    "    # Calculate BERT predictions\n",
    "    \n",
    "    predictions_logreg = model_logregression_new.predict_proba(corpus_test)\n",
    "\n",
    "# Destructure probabilities for class_1 and class_2\n",
    "    predictions_class_0 = predictions_logreg[:, 0]\n",
    "    predictions_class_1 = predictions_logreg[:, 1]\n",
    "    \n",
    "    # Create a DataFrame containing the test samples and BERT predictions\n",
    "    results = pd.DataFrame({\n",
    "    'logreg_0': predictions_class_0,\n",
    "    'logreg_1': predictions_class_1\n",
    "    })\n",
    "    # Create a DataFrame containing the test samples and BERT predictions\n",
    "  \n",
    "    # Save the DataFrame to a CSV file\n",
    "    results.to_csv(LOGREG_PREDICTION_FILE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Build Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(min_samples_leaf=20)\n",
    "model_dtc = PipelineWrapper(model, vectorizer_tf_idf, corpus_test, corpus_train, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dtc.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dtc.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_dtc.predict_proba([corpus_test[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC_PREDICTION_FILE = 'predictions_dtc.csv'\n",
    "\n",
    "file_exists = os.path.exists(DTC_PREDICTION_FILE)\n",
    "\n",
    "if not file_exists:\n",
    "    # Calculate DTC predictions\n",
    "    predictions_DTC = model_dtc.predict_proba(corpus_test)\n",
    "\n",
    "# Destructure probabilities for class_1 and class_2\n",
    "    predictions_class_0 = predictions_DTC[:, 0]\n",
    "    predictions_class_1 = predictions_DTC[:, 1]\n",
    "    \n",
    "    # Create a DataFrame containing the test samples and BERT predictions\n",
    "    results = pd.DataFrame({\n",
    "    'dtc_0': predictions_class_0,\n",
    "    'dtc_1': predictions_class_1\n",
    "    })\n",
    "    results.to_csv(DTC_PREDICTION_FILE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Build BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../../FineTunedBERT/Argument/26062217\")\n",
    "# Load model\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"../../FineTunedBERT/Argument/26062217\")\n",
    "model_bert = pipeline('text-classification', model=loaded_model, tokenizer=tokenizer, max_length=512, truncation=True, top_k=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_PREDICTION_FILE = 'prediction_BERT.csv'\n",
    "\n",
    "file_exists = os.path.exists(BERT_PREDICTION_FILE)\n",
    "\n",
    "if not file_exists:\n",
    "    # Calculate BERT predictions\n",
    "    raw_predictions_bert = model_bert.predict(corpus_test)\n",
    "\n",
    "    # Destructure probabilities for class_1 and class_2\n",
    "    predictions_class_0 = [pred[0]['score'] if pred[0]['label'] == 'LABEL_0' else pred[1]['score'] for pred in raw_predictions_bert]\n",
    "    predictions_class_1 = [pred[0]['score'] if pred[0]['label'] == 'LABEL_1' else pred[1]['score'] for pred in raw_predictions_bert]\n",
    "\n",
    "    # Create a DataFrame containing the test samples and BERT predictions\n",
    "    results = pd.DataFrame({\n",
    "        'bert_0': predictions_class_0,\n",
    "        'bert_1': predictions_class_1\n",
    "    })\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    results.to_csv(BERT_PREDICTION_FILE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Analyze With SHAP\n",
    "\n",
    "We know that the Logistic Regression Performs better than the Decision Tree Classifier.\n",
    "To further investigate why thats the case we need to look into the models.\n",
    "\n",
    "In the following i will do 3 things.\n",
    "\n",
    "Per Model:\n",
    "1. Most Positive/Negative Words\n",
    "2. Investigate the Most Wrong Positive and Negative prediction\n",
    "\n",
    "Model Comparision:\n",
    "1. Investigate biggest Prediction gap accross the models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what is a claim?\n",
    "The guidelines provided to the annotators present mainly 3 criteria, which all have to be met for a positive label.\n",
    "\n",
    "1. The sentence must clearly support or contest the topic, and not simply be neutral.\n",
    "2. It has to be coherent and stand mostly on its own.\n",
    "3. It has to be convincing, something you could use to sway someone's stance on the topic (a claim is not enough, it has to be backed up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = shap.maskers.Text(tokenizer=r\"\\W+\") # this will create a basic whitespace tokenizer\n",
    "# explainer_logreg = shap.Explainer(model_logregression_new.predict_proba , masker)\n",
    "explainer_logreg = shap.Explainer(model_logregression_new.predict_proba, masker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Optimazation\n",
    "\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "shap_values_file = \"shap_values_logreg.pkl\"\n",
    "\n",
    "# Überprüfen, ob die Datei existiert\n",
    "if not os.path.exists(shap_values_file):\n",
    "    # Berechnen Sie die SHAP-Werte, wenn die Datei nicht existiert\n",
    "    shap_values_logreg = explainer_logreg(corpus_test)\n",
    "\n",
    "    # Speichern Sie die SHAP-Werte in einer Datei\n",
    "    with open(shap_values_file, \"wb\") as f:\n",
    "        pickle.dump(shap_values_logreg, f)\n",
    "else:\n",
    "    # Laden Sie die SHAP-Werte aus der Datei, wenn sie existiert\n",
    "    with open(shap_values_file, \"rb\") as f:\n",
    "        shap_values_logreg = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Investigate the biggest misclassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.disagreements import get_misclassifications\n",
    "logreg_misclassifications = get_misclassifications(y_test,LOGREG_PREDICTION_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"total samples\", len(y_test))\n",
    "print(\"total misclassifications\", len(logreg_misclassifications))\n",
    "\n",
    "# count number of samples for label 1 and label 0\n",
    "label_0 = y_test.count(0)\n",
    "label_1 = y_test.count(1)\n",
    "\n",
    "\n",
    "\n",
    "# print number of samples for label 1 and label 0\n",
    "\n",
    "print(\"Number of samples for label 0: \", label_0)\n",
    "print(\"Number of samples for label 1: \", label_1)\n",
    "\n",
    "# count missclassifications for class 0 and for class 1\n",
    "\n",
    "logreg_misclassifications_0 = logreg_misclassifications[(logreg_misclassifications['label'] == 0)]\n",
    "logreg_misclassifications_1 = logreg_misclassifications[(logreg_misclassifications['label'] == 1)]\n",
    "\n",
    "# print the number of missclassifications for class 0 and for class 1\n",
    "print(\"Number of missclassifications for class 0: \", len(logreg_misclassifications_0))\n",
    "print(\"Number of missclassifications for class 1: \", len(logreg_misclassifications_1))\n",
    "\n",
    "\n",
    "# calculate the percentage of missclassifications for class 0 and for class 1\n",
    "percentage_0 = len(logreg_misclassifications_0) / label_0\n",
    "percentage_1 = len(logreg_misclassifications_1) / label_1\n",
    "\n",
    "# print the percentage of missclassifications for class 0 and for class 1\n",
    "print(\"Percentage of missclassifications for class 0: \", percentage_0)\n",
    "print(\"Percentage of missclassifications for class 1: \", percentage_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find first with label = 0\n",
    "logreg_misclassifications[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wrong_positive_index = 797"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Most Wrong Positive Classification\n",
    "\n",
    "Most wrong review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logreg_misclassifications.loc[most_wrong_positive_index])\n",
    "print(corpus_test[most_wrong_positive_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = shap_values_logreg[most_wrong_positive_index, :, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so its 0.5 behind the real label.\n",
    "But Why lets investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_shap_explainer.signal_words import highlight_signal_words\n",
    "shap.plots.text(explanation)\n",
    "shap.plots.waterfall(explanation)\n",
    "\n",
    "highlight_signal_words(explanation, round_shap_values=3, top_words=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpretation:\n",
    "\n",
    "This is a claim but the model is not able to capture the classification correctly. Because it weights the no-claim words (like: is its is) more than the claim words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Most Wrong Negative Classification\n",
    "\n",
    "Most wrong review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find first with label = 0\n",
    "most_wrong_negative_index = logreg_misclassifications[logreg_misclassifications['label'] == 0].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logreg_misclassifications.loc[most_wrong_negative_index])\n",
    "print(corpus_test[most_wrong_negative_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap_values_logreg[most_wrong_negative_index, : , 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(explainer)\n",
    "print(len(explainer))\n",
    "shap.plots.waterfall(explainer)\n",
    "highlight_signal_words(explainer,round_shap_values=3,top_words=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation:\n",
    "\n",
    "THis is a neutral observation ergo no claim but the words:\n",
    "- right\n",
    "- libertarian\n",
    "- ref\n",
    "- prostituion\n",
    "- supports\n",
    "- bear\n",
    "\n",
    "have high positive impact on the classification.\n",
    "\n",
    "Furthermore the model is not really learning why this is not a claim it dont knows that "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Global Interpretation Most Positive and negative Words\n",
    "\n",
    "To understand how the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_global_explanation(shap_values_logreg[:,:,1], num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_global_shap_distribution(shap_values_logreg[:,:,1], threshold=0.02)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "The most important Words Contributing to a Claim Classification:\n",
    "\n",
    "1. opposes\n",
    "2. ban\n",
    "3. supports\n",
    "4. opposed\n",
    "\n",
    "with the highest shap value of 0.2 of oppeses\n",
    "\n",
    "The most important Words Contributing to a no-claim classification:\n",
    "\n",
    "1. Private\n",
    "2. free \n",
    "3. include\n",
    "4. other\n",
    "\n",
    "with the highest shap value of -0.1 \n",
    "\n",
    "you can see that the model pays more attention to the words that are characteristic for a claim classification and less to words that are non-claim charateristic.\n",
    "\n",
    "\n",
    "but maybe as soon as the main claim classification words are not present it struggles to identify a sample as claim.\n",
    "\n",
    "\n",
    "This fits to our previous observation were i looked for the misclassified classes and the biggest misclassifications.\n",
    "Percentage of missclassifications for class 0:  0.030917874396135265\n",
    "Percentage of missclassifications for class 1:  0.787701317715959"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analyze with SHAP: DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = shap.maskers.Text(tokenizer=r\"\\W+\") # this will create a basic whitespace tokenizer\n",
    "# explainer_logreg = shap.Explainer(model_logregression_new.predict_proba , masker)\n",
    "explainer_dtc = shap.Explainer(model_dtc.predict_proba, masker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Optimazation\n",
    "\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "shap_values_file = \"shap_values_dtc.pkl\"\n",
    "\n",
    "# Überprüfen, ob die Datei existiert\n",
    "if not os.path.exists(shap_values_file):\n",
    "    # Berechnen Sie die SHAP-Werte, wenn die Datei nicht existiert\n",
    "    shap_values_dtc= explainer_dtc(corpus_test)\n",
    "\n",
    "    # Speichern Sie die SHAP-Werte in einer Datei\n",
    "    with open(shap_values_file, \"wb\") as f:\n",
    "        pickle.dump(shap_values_dtc, f)\n",
    "else:\n",
    "    # Laden Sie die SHAP-Werte aus der Datei, wenn sie existiert\n",
    "    with open(shap_values_file, \"rb\") as f:\n",
    "        shap_values_dtc = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Investigate Biggest Misclassifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_misclassifications = get_misclassifications(y_test, DTC_PREDICTION_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"total samples\", len(y_test))\n",
    "print(\"total misclassifications\", len(dtc_misclassifications))\n",
    "\n",
    "# count number of samples for label 1 and label 0\n",
    "label_0 = y_test.count(0)\n",
    "label_1 = y_test.count(1)\n",
    "\n",
    "\n",
    "\n",
    "# print number of samples for label 1 and label 0\n",
    "\n",
    "print(\"Number of samples for label 0: \", label_0)\n",
    "print(\"Number of samples for label 1: \", label_1)\n",
    "\n",
    "# count missclassifications for class 0 and for class 1\n",
    "\n",
    "dtc_misclassifications_0 = dtc_misclassifications[(dtc_misclassifications['label'] == 0)]\n",
    "dtc_misclassifications_1 = dtc_misclassifications[(dtc_misclassifications['label'] == 1)]\n",
    "\n",
    "# print the number of missclassifications for class 0 and for class 1\n",
    "print(\"Number of missclassifications for class 0: \", len(dtc_misclassifications_0))\n",
    "print(\"Number of missclassifications for class 1: \", len(dtc_misclassifications_1))\n",
    "\n",
    "\n",
    "# calculate the percentage of missclassifications for class 0 and for class 1\n",
    "percentage_0 = len(dtc_misclassifications_0) / label_0\n",
    "percentage_1 = len(dtc_misclassifications_1) / label_1\n",
    "\n",
    "# print the percentage of missclassifications for class 0 and for class 1\n",
    "print(\"Percentage of missclassifications for class 0: \", percentage_0)\n",
    "print(\"Percentage of missclassifications for class 1: \", percentage_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_misclassifications[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1.2 Most Wrong Positive classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wrong_positive_index = 1073\n",
    "explanation = shap_values_dtc[most_wrong_positive_index, :, 1]\n",
    "print(dtc_misclassifications.loc[most_wrong_positive_index])\n",
    "print(corpus_test[most_wrong_positive_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(explanation)\n",
    "shap.plots.waterfall(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_signal_words(explanation,round_shap_values=3,top_words=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interpretation:\n",
    "\n",
    "Why is this a claim?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1.2 Most Wrong Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wrong_negative_index = dtc_misclassifications[dtc_misclassifications['label'] == 0].index[0]\n",
    "most_wrong_negative_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = shap_values_dtc[most_wrong_negative_index, :, 1]\n",
    "print(dtc_misclassifications.loc[most_wrong_negative_index])\n",
    "print(corpus_test[most_wrong_negative_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(explanation)\n",
    "shap.plots.waterfall(explanation)\n",
    "highlight_signal_words(explanation,round_shap_values=3,top_words=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpretation\n",
    "Thats not a claim.\n",
    "\n",
    "but the model learned that the word \"that\" is introduction for a claim that is not the case.\n",
    "in this case its only an introduction for a study..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Global Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_global_explanation(shap_values_dtc[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_global_shap_distribution(shap_values_dtc[:,:,1], threshold=0.02)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpretation\n",
    "\n",
    "you can see shap values that contributing to claim are way more above the threshold than the shap values contributing against a claim.\n",
    "\n",
    "What are the impacts of this?\n",
    "\n",
    "-> the models classifies way more as claim than as no-claim because it has way more words that contribute to class 1.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. SHAP for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "In summary, the reason your code is not using the GPU is that the SHAP library does not support GPU acceleration for BERT models. Unfortunately, there is no direct solution to this issue. You can try looking for alternative libraries or methods that support GPU acceleration for BERT or similar text models.\n",
    " \"\"\"\n",
    "\n",
    "# Performance Optimazation\n",
    "masker = shap.maskers.Text(tokenizer=r\"\\W+\") # this will create a basic whitespace tokenizer\n",
    "# explainer_logreg = shap.Explainer(model_logregression_new.predict_proba , masker)\n",
    "explainer_bert = shap.Explainer(model_bert, masker)\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "shap_values_file = \"shap_values_bert.pkl\"\n",
    "\n",
    "# Überprüfen, ob die Datei existiert\n",
    "if not os.path.exists(shap_values_file):\n",
    "    # Berechnen Sie die SHAP-Werte, wenn die Datei nicht existiert\n",
    "    shap_values_bert= explainer_bert(corpus_test[:100])\n",
    "\n",
    "    # Speichern Sie die SHAP-Werte in einer Datei\n",
    "    with open(shap_values_file, \"wb\") as f:\n",
    "        pickle.dump(shap_values_bert, f)\n",
    "else:\n",
    "    # Laden Sie die SHAP-Werte aus der Datei, wenn sie existiert\n",
    "    with open(shap_values_file, \"rb\") as f:\n",
    "        shap_values_bert = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Investigate biggest Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_misclassifications = get_misclassifications(y_test, BERT_PREDICTION_FILE)\n",
    "dtc_misclassifications[:50]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.2 Investigate biggest Misclassifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wrong_positive_index = 210\n",
    "\n",
    "explanation = explainer_bert([corpus_test[most_wrong_positive_index]])\n",
    "explanation = explanation[0, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtc_misclassifications.loc[most_wrong_positive_index])\n",
    "print(corpus_test[most_wrong_positive_index])\n",
    "shap.plots.text(explanation)\n",
    "shap.plots.waterfall(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_signal_words(explanation,round_shap_values=3,top_words=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "This is a claim but BERT fails to classify it as such:\n",
    "claim: death warns against such misplaced values and condemns the practice of censorship as well as demonstrating there can be value in a show often dismissed as juvenile and immature\n",
    "Prove: like south park or terrance and phillip ref"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1.2 Biggest Negative misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wrong_negative_index = 1024\n",
    "\n",
    "explanation = explainer_bert([corpus_test[most_wrong_negative_index]])\n",
    "explanation = explanation[0, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtc_misclassifications.loc[most_wrong_negative_index])\n",
    "print(corpus_test[most_wrong_negative_index])\n",
    "shap.plots.text(explanation)\n",
    "shap.plots.waterfall(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_signal_words(explanation,round_shap_values=3,top_words=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpretation\n",
    "\n",
    "The model learned that \"study concluded\" are words that lead to a claim.\n",
    "I think its not labeled as a claim because its a neutral view on a study"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Global Explanation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_global_explanation(shap_values_bert[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_global_shap_distribution(shap_values_bert[:,:,1], threshold=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.disagreements import get_disagreements\n",
    "\n",
    "dtc_bert_sorted_by_diff,logreg_bert_sorted_by_diff,logreg_dtc_sorted_by_diff = get_disagreements(y_test,BERT_PREDICTION_FILE, LOGREG_PREDICTION_FILE, DTC_PREDICTION_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_bert_sorted_by_diff.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_bert_sorted_by_diff.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 1253 is intresting because there is a big differenc between the predictions of the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dtc_log_reg_biggest_difference_index = 1253\n",
    "corpus_test[bert_dtc_log_reg_biggest_difference_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_explanation = explainer_dtc([corpus_test[bert_dtc_log_reg_biggest_difference_index]])\n",
    "logreg_explanation = explainer_logreg([corpus_test[bert_dtc_log_reg_biggest_difference_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DTC Explanation')\n",
    "highlight_signal_words(dtc_explanation[0,:,1],round_shap_values=3,top_words=6)\n",
    "shap.plots.text(dtc_explanation[0,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('logreg Explanation')\n",
    "highlight_signal_words(logreg_explanation[0,:,1],round_shap_values=3,top_words=6)\n",
    "shap.plots.text(logreg_explanation[0,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_explanation = explainer_bert([corpus_test[bert_dtc_log_reg_biggest_difference_index]])\n",
    "bert_explanation = bert_explanation[0, :, 1]\n",
    "print('BERT Explanation')\n",
    "highlight_signal_words(bert_explanation,round_shap_values=3,top_words=6)\n",
    "shap.plots.text(bert_explanation)\n",
    "shap.plots.waterfall( bert_explanation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "\n",
    "why is this a claim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HUGGING_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
